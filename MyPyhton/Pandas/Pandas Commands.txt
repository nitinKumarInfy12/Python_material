######################## Environments ##################################################
- Go to Pandas commmand line - > 
	type$ conda info --envs
# it gives the list of existing pands environments, * indicates teh active enviornment

# update canda version or any library
type$ conda update conda
type$ conda update <library name>

# update all teh libraries within the environment
type$ conda update --all

# create a new enviornment
type$ conda create --name <environment name of choice>

# to activate the enviropnemt
type$ conda activate <environment name of choice>

# to deactivate an active environment
type$ conda deactivate

# remove an environment
type$ conda remove --name <environment name of choice>

# remove environment with all teh libraries in it
type$ conda remove --name <environment name of choice> --all

# install one or more libraries in the enviroenment
type$ conda install <library name> <library name2>
# conda install pandas jupyter bottleneck numexpr matplotlib

######################## jupyter notebook ##################################################
# start jupyter notebook
- Go to Pandas commmand line - > 
- activate the environment which has teh jupyter installed in that
    type$ jupyter notebook
#it opens a server and a folder explorer you are currenly in default browser, chrome or IE

# run the code in jupyter
Shift + Enter

# run the code when you are in teh last cell and dont want to add any more cell to jupyter 
ctrl + Enter

# add new lines to the notebook
ESC+ B

# hide output
output = None

# get pandas version
pandas.__version__


# shut down jupyter notebook
1. Save the work
2. close the tab
3. go to the main tab of jupyter
4. click on Running Tab, click on ShutDown button for the notebook_name
5. Close the browser tab
6. go back to terminal press CTRL+C two times
7. once returns to nomal command line with flashing cursor, close the terminal.
# all done now.


######################## Cell Type shortcuts ##################################################
- Press Esc to go to command mode
- press y to change cell type to Python
- press m to change cell type to markdown 
- press  h for help, it lists all the shortcuts
- press x to cut teh cell
- press b to insert a cell below
- press a to cerate a cell above




######################## Import Libraries in jupyter notebooks ##################################################
import <library name>
- press Shift + Entr

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline


######################## Series ##################################################
# it is one dimensional labeled array, it should have same data type to all list items in it
# Series object can be created using Series() constructor method from pandas
# pd.Series()
# index can be of any data type in series.
# object type in series is actually string, rest all data types are as usual
# Key or Index in Pandas series objects, can be nonuniue/duplicate unlike in python dictionaries
# Tab : completes the method, attibute keyword
# hold Shift + Tab    : brings teh documentation of teh function
# opandas reads any file by default s data frame even if it has only 1 column. use "squeeze=TRUE" to change it as a seried object than data frame onbject
# Column name becomes teh name os teh series, if iported file with 1 column

pokemon = pd.read_csv("pokemon.csv", usecols=["Pokemon"], squeeze=True)
google = pd.read_csv("google_stock_price.csv", squeeze=True)

# read_csv() , head(), tail() methods
# read_csv() : returns teh data file in form of data frame unless changed to series
# head() : returns first 5 rows by default from the source file, can be changed by providing desired number of rows .head(10)
# tail() : opposite to head() method

# python builtin functions works well with pandas series.
# len(series_object) : number of items in series
# type(series_object) : pandas.core.series.Series
# dir(series_object): give the list of available metrhods, attributes to series object
# sorted(series_object): sorts series
# list(series_object) : converts series into paython list
# dict(series_object): converts series  in dictionary
# or series_object.to_dict()  also converts series  in dictionary
# max(series_object) , min(series_object) : ruturns the maximu or minimum values from teh series.
# in : checks if series value or index exists in series
# shift + Tab in the () of any method provides teh list of method parameters

# inplace parameter : when set to True, it overwrites the objects in teh original valriable

# fetch series value using index series_object[index_position]
# series_object[index_position] : returns 1 series element
# series_object[index_position1, index_position2, index_position3] : returns a new series of all the mentions index item values
# series_object[: index_position] : begining to index position
# series_object[index_position1 : index_position2] : index positio1 to index_position2
# series_object[-index_position : ] last 30 index elements
# series_object[-index_position10 : -index_position2] : returns from last index10 to last index2

# assign a coulmn from source as index to series
pd.read_csv("pokemon.csv", index_col="Pokemon") # its still a dataframe, use squeeze to convert into series
pd.read_csv("pokemon.csv", index_col="Pokemon", squeeze=True)

# now series elements can be access by index label also as well as index number
series_object[index1]  or series_object["index_label"]
pokemon[["Yveltal", "Hoopa","Venusaur"]] # retuns the series of these particular index label items
pokemon["Yveltal" : "Hoopa"] # returns series of elements from indexlabel1 to including indexlabel2

#.get()
pokemon.get(key = "Digion", default = "This is not a pokemon") # retunrs "his is not a pokemon" if element doesnot exists

# mathematical methods
series_object.count()  # returns the valid values from series, it ignores null values
series_object.sum(), series_object.mean(), series_object.min(), series_object.max()
series_object.mode(), series_object.median()
series_object.describe() : proves teh statistiv=cs of teh series


# .idxmax() and .idxmin() : returns teh index position of maximum / minimum value in teh series
series_object.idxmax() , series_object.idxmin()

# .value_counts() method : its similar to excel pivot table
# it returns the count of each value in teh series

# apply() method : it calls a function on each value of teh series
# lets say to apply some custom method on each element within series
series_object.apply(custom_function{without parenthesis})

# lambda : anonymous function, does not have a name
series_object.apply(lambda some_var_name : function body using teh variable)
#Ex: this function will add 1 to all series values
google.apply(lambda stck_prc : stck_prc + 1)


# map() method : maps teh values of a series to another collection of data
# its like vlookup in excel
series_object_1.map(series_object) 
# its will look for each value from series 1 and search in index of seriies 2, and will return a new
# series with index from series 1 and matching values from series 2
# map method also works the same with dictionaries
dict_1.map(dict_2)



######################## Data Frame ##################################################
# Data frame is a 2 simensional object, resembles to table

#### shared methods / attributes between data frames and series
head()/tail()
# attribute: index , values, shape , dtypes

# data frames attributes
columns , axis (combination of index and columns attributes)
info (summary of data frame), 

dataframe_obj.info()
dataframe_obj.dtypes

# data frame methods
get_dtype_counts()



# import single colum in data frame
Data_Frameobject.Column_name # it only work if column name does not have speaces or special characters
# or
Data_Frameobject[Column_name] # it will work even if column name has speaces or special characters

# import 2 or more columns from data frame
Data_Frameobject[[Column_name1, Column_name2 , Column_name3]] : it returns a new data frame


# Add new columns t data frame
# old Way
dataframe_obj["Column name"] = "some value" # colum will be addedd with the value to each dataframe rows
data

# new way
dataframe_obj.insert(index, Column_name, Value,allow_duplicates=False/True )
 

######################## Broadcasting operations ##################################################

dataframe_obj["Column_name"].add(5) # adds 5 to all values in teh colum
dataframe_obj["Column_name"] + 5 # it also adds 5 to all values in teh colum

dataframe_obj["Column_name"].sub(5) # subtracts 5 to all values in teh colum
dataframe_obj["Column_name"] - 5 # it also subtracts 5 to all values in teh colum


dataframe_obj["Column_name"].mul(5) # multiplies 5 to all values in teh colum
dataframe_obj["Column_name"] * 5 # it also multiplies 5 to all values in teh colum

new_Sal = dataframe_obj["Salary"].div(1000000)
dataframe_obj.insert(5,"Salary in milions", new_Sal,allow_duplicates=True)

# value_counts() method. it is only allied to series not on data frames
# to use with dataframes, need to pull one colum as series and apply the value_counts() meathod on it.

# deal will null values
# remove rows/columns if any/all column has null value : dropna()/dropna(how = "all")
dataframe_obj.dropna() # removes row if it has any null value in any column
dataframe_obj.dropna(how="all") # removes row if has null value in all column

dataframe_obj.dropna(axis = 1) # removes the column if it has any null value
dataframe_obj.dropna(axis = "columns") # removes the column if it has any null value

dataframe_obj.dropna(subset = ["Salary"]) # it will drop only that row which has null  in Salary column
dataframe_obj.dropna(subset = ["Salary", "College"])

# replace Null with some specific value
# fillna() on the dataframe
dataframe_obj.fillna(0) # repplaces all nulls with 0 in all columns/rows of dataframe
dataframe_obj["Salary"].fillna(0, inplace=True) # replaces null with 0 only in the Salary column in teh dataframe
dataframe_obj["College"].fillna("Not Applicable", inplace=True) # replaces null with "Not Applicab;e" only in the College column in the dataframe


# astype() method : it coverts the data type of the column. it can be called on a series of a data frame
dataframe_obj.dtypes # provides teh data type of each column in dataframe
dataframe_obj.info() $ provides details of te dataframe

dataframe_obj["salary"] = dataframe_obj["Salary"].astype("int")
dataframe_obj["check"] = dataframe_obj["check"].astype("bool")
dataframe_obj["Gender"] = dataframe_obj["Gender"].astype("category")
# converting to correct datatype also saves space/memory

# category data type : it is ideal when you hav small set of unique values in a dataframe
dataframe_obj["Position"] = dataframe_obj["position"].astype("category")


# soritng a data frame
dataframe_obj.sort_values("Column_Name") # sorts the data frame by values in column_name
dataframe_obj.sort_values("Column_Name", ascending=True, na_position='last') 
# sorts the data frame by values in column_name and puts teh null values in last

dataframe_obj.sort_values(["Column_Name1","Column_Name1"], ascending=True) 
# sorts the data frame by values in column1 and column 2 in ascending order

dataframe_obj.sort_values(["Column_Name1","Column_Name1"], ascending=[True,False]) 
# sorts the data frame by values in column1 in ascendeing order and column 2 in descending order

# sort_index() sort data frame by inddex
dataframe_obj.sort_index()


# rank() method rank . is called on single series. provides rank of each value
# get rid of null value to work with rank()

dataframe_obj["column"].rank() # be default its sorted in descending
dataframe_obj["column"].rank(ascendeing=True).astype("int")




######################## data frame 2 ##################################################

# convert to date time
# to_datetime(series)
pd.to_datetime(dataframe_obj["Column"])  
#note: it replaces teh nullvalues with current date time

dataframe_obj = pd.read_csv("file.csv")
dataframe_obj[column1] = pd.to_datetime(dataframe_obj[column1]) # converts column1 data type from object/string to datetime
dataframe_obj[column2] = pd.to_datetime(dataframe_obj[column2])# converts column1 data type from object/string to datetime

# Shortcut for teh same
dataframe_obj = pd.read_csv("file.csv", parse_dates=[column1,column2])



# filtering , can be used on series from the data frame by creating boolean series
dataframe_obj["Gender"] == "Male" # it returns a new series of booleans

# apply the filter in on the dataframe
dataframe_obj[dataframe_obj["Gender"]=="Male"] 
# it will only return teh rows that have Male value in teh Gender column

# or
mask(variable_name) = dataframe_obj["Gender"] == "Male"
dataframe_obj[mask]  # use the variable instead of whole formula in teh square bracket

# if a coulmn is already boolean type
dataframe_obj[dataframe_obj[bool_column]]

# coparision operator
==, != , >, < , <= , >= , &, |

# filtering on multiple conditions, using multiple variables
mask1 = dataframe_obj["Gender"] == "Male"
mask2 = dataframe_obj["Team"] != "Marketing"

# And operataor
dataframe_obj[mask1 & mask2] # will filter when both the variable values are True

# or operataor
dataframe_obj[mask1 | mask2] # will filter when either of the variable value is True

# both and and or together in filetr
mask3 = dataframe_obj["First Name"] != "Robert"

dataframe_obj[(mask1 & mask2) | mask3] 

# isin() method : comparing ultiple values in a single series
mask1 = dataframe_obj["First Name"] != "Robert"
mask2 = dataframe_obj["First Name"] != "Julio"
mask3 = dataframe_obj["First Name"] != "Jack"

dataframe_obj[mask1 | mask2 | mask3] # provides data frame if first name mathes any of these 3 values

# this can be done usong isin() method without duplicating the code.
mask4 = dataframe_obj["First Name"].isin(["Robert","Julio","Jack"])

dataframe_obj[mask4]  # its doing the same thing as creatign 3 different variable, one for each value

# isnull() , notnull() methods to check values in a series from a data frame
mask = dataframe_obj[Column].isnull()
mask = dataframe_obj[Column].notnull()

# between() : to comapre between some range
dataframe_obj[dataframe_obj[Column].between(lower_val, higher_val)] # both the values are inclusive (<=, >=)

# duplicated() : to check the duplicate values in a series
dataframe_obj[dataframe_obj[Column].duplicated()] # it wont mark teh first occurence as duplicate, but from teh second occurance of teh same value
dataframe_obj[dataframe_obj[Column].duplicated(keep = "last")] # it wont mark teh last occurence as duplicate, but from teh second occurance of teh same value

# remove duplicate rows
dataframe_obj[dataframe_obj[Column].duplicated(keep = False)] # it displays only those rows, that are duplicate

# negate the result , True to False or vice versa using ~ in front
mask = ~dataframe_obj[Column].duplicated(keep = False)
dataframe_obj[mask] # it displays only those rows, that are not duplicate and unique

# drop_duplicates() : on dataframe
df.drop_duplicates() :# it will remove the rows if they are identical
df.drop_duplicates(subset = ["Column1","Column2","Column3"], keep="first") :# it will remove the rows if they are identical

df.drop_duplicates(subset = ["Column1"], keep=False) :# it will removeall the instances of duplicate, so it might return empty data frame

df.drop_duplicates(subset = ["Column1","Column2","Column3"], keep="first", inplace=True)


# unique() and nunique() methods

dataframe_obj[column_name].unique() # retuns an array of unique value
# it includes null in teh result set

dataframe_obj[column_name].nunique() # give the number of unique values in that column
# it drops the null values from count

dataframe_obj[column_name].nunique(dropna=False)
# give the number of unique values including null also from the column


######################## Data frame 3 ##################################################

# index can be set while impporting the file, within read_csv( index_col="Column1")
# or using the set_index() as well if not already set an index during the import process in read_csv

# set_index() : sets any column from teh data frame as index for teh data frame
dataframe_obj.set_index("Column1")

dataframe_obj.set_index("Column1") # it will drop teh existing index and set colum2 as index

# reset_index(): it converts teh index as a column to dataframe if drop parameter is not specified
dataframe_obj.reset_index() # it bring the index as colum to  teh data frame and sets normal numeric index

dataframe_obj.reset_index(drop=False) # it doesnot bring the index as colum to  teh data frame and sets normal numeric index
dataframe_obj.reset_index(drop=False, inplace=True) 



# loc[] :retrieve rows by index label with loc[]
dataframe_obj.loc["index label"] # it will retuen a series with values from each column
# if index label doesnot exists , it throws error
# if duplicate index available,, it returns data frame of all values

dataframe_obj.loc["index label_1" : index label_2] # it return data frame including all values from index_label_1 to 2
dataframe_obj.loc["index label_1" : ] # all strating from index_1

dataframe_obj.loc[["index label_1","index label_2"]] # brings these 2 in data frame
dataframe_obj.loc[["index_1", "index_2","Wrong_index"]] # brings all 3 and wont throw error if any of the index exists

"Wrong_index" in dataframe_obj.index() # returns false as teh index doesnot exists

# second argument to teh loc[] , column name/list
dataframe_obj.loc[["index_1", "Column Name"] # returns teh intersection
dataframe_obj.loc[["index_1", ["Column Name": "Column Name2"]]
# fethce intersection from all colums

dataframe_obj.loc[["index_1", ["Column Name", "Column Name2", "Column Name3"]]
# fetches intersection for selected columns


# iloc[] :retrieve by index location
dataframe_obj.iloc[index_pos] # retuns the series for teh index row with all column values
dataframe_obj.iloc[index_pos : index_pos_2]
dataframe_obj.iloc[ : index_pos_2]
dataframe_obj.iloc[index_pos : ]

dataframe_obj.iloc[[index_pos1, index_pos2, index_pos3, index_pos4, index_pos5]]

# second argument to teh iloc[] , column index/list of index
dataframe_obj.iloc["index_1", "Column_index"] # returns teh intersection
dataframe_obj.iloc["index_1", "Column_index1": "Column_index2"]
# fethce intersection from all colums

dataframe_obj.iloc[["index_1", ["Column_index", "Column_index2", "Column_index3"]]
# fetches intersection for selected columns


# Catch-All .ix[] : it combines both loc[] and iloc[] methods
dataframe_obj.ix[["index_label_1", "index_label_2","Wrong_index"]]
dataframe_obj.ix[index_pos : index_pos_2]
# whenever deal wil index positions, last does not get included in case or range

dataframe_obj.ix[[index_pos1, index_pos2, index_pos3, index_pos4, index_pos5]]

# second argument to teh ix[] , column name/index/list of index/name
dataframe_obj.ix["index_1", "Column_index"] # returns teh intersection
dataframe_obj.ix["index_1", "Column_name1": "Column_name2"]
# fethce intersection from all colums

dataframe_obj.ix[["index_name", ["Column_name", "Column_name2", "Column_name3"]]
# fetches intersection for selected columns


# set specific value for a row or column in data frame
# single value
dataframe_obj.ix["index_pos/name", "Column"] = "new value"

# multipe values
dataframe_obj.ix["index_pos/name", ["Column1","Column2","Column3"]] = ["new_val1","new_val2","new_val3"]

# Update ,multiple occurence of a value with new value
mask = dataframe_obj["Column1"] == "Existing Value"
dataframe_obj.ix[mask, "Column1"] = "New Value"

dataframe_obj[dataframe_obj["Column1"] = "Existing Value"] # creates a copy of data frame 
dataframe_obj.ix[dataframe_obj["Column1"] = "Existing Value"] # displays the set of data from tha actuak dataframe

# rename index or column in data frame
# rename()
dataframe_obj.rename(columns = {"Existing Column1 Name" : "New Column1 Name", 
                                "Existing Column2 Name" : "New Column2 Name"}, inplace=True)

dataframe_obj.rename(index = {"Existing Index1 label" : "New Index1 label", 
                                "Existing Index2 label" : "New Index2 label"}, inplace=True)
                                
                                
# or ther way to change the column name
dataframe_obj.columns = ["Column1_old Name","Column2_old Name","Column3_new Name","Column4_new Name"]


# delete rows columns from data frame
# drop()

dataframe_obj.drop("Index label or index number")
dataframe_obj.drop(["Index label1","Index label2","Index label3"])

dataframe_obj.drop("Column1", axis=1)
# axis=0 means rows or index: axis=none, axis=0, axis="rows", axis="index" => all means same
# axis=1 means columns : axis=1 or axis="columns" means same


dataframe_obj.drop(["Column1","Column2","Column3"], axis=1 , inplace=True)

# pop() : accepts a series as parameter and dont need inplace, also it returns the series 
dataframe_obj.pop("Column name")

vColumn = dataframe_obj.pop("Column name")
# it drops teh column from teh data farame and saves in teh variable the same series

# using python del keyword: need to pass a series to it
del dataframe_obj["Column"] # this column will be deleted from teh data frame

# extract randm sample from teh data frame
# sample()
dataframe_obj.sample() # it returns a single random record from data frame
dataframe_obj.sample(n=5) # it returns 5 random records from data frame

dataframe_obj.sample(frac = .25) # it returns 25% random records from data frame

dataframe_obj.sample(n=5, axis=1) # it returns 5 random columns with all rows from data frame
dataframe_obj.sample(n=5, axis="columns") # it returns 5 random columns with all rows from data frame


# extract smallest / largest values from column
# nlargest() / nsmallest()
dataframe_obj.nsmallest(3, columns = "Column1") # gives top 3 rows based on column1

dataframe_obj.nlargest(3, columns = "Column1") # gives last 3 rows based on column1

# it can be directly applied on a series
dataframe_obj["Column"].nlargest(8) # gives the top 8 records from the series
dataframe_obj["Column"].nsmalles(8) # it gives the top 8 records from the series

# filter with where() method :it returns teh original data frame, fills not matching records with null
mask = dataframe_obj["Column"] == "Some Value" # boolean list
dataframe_obj[mask]

dataframe_obj.where(mask) # it returns teh original data frame, fills not matching records with null
# or
dataframe_obj.where(dataframe_obj["Column"] == "Some Value")


# filter data with query(), it can be called on dataframe
# columns names must not have spaces in dataframe columns
dataframe_obj.columns = [column_name.replace(" ", "_") for column in dataframe_obj.columns]

dataframe_obj.query('Column_Name =="Some Value"')
# or
dataframe_obj.query("Column_Name =='Some Value'")

dataframe_obj.query("Column_Name1 =='Some Value' and Column_Name2 =='Some val'")
dataframe_obj.query("Column_Name1 =='Some Value' or Column_Name2 =='Some val'")

dataframe_obj.query("Column_Name1 > 10")
dataframe_obj.query("Column_Name1 != 10")
dataframe_obj.query("Column_Name1 in ['Value1','Value2','Value3']")
dataframe_obj.query("Column_Name1 not in ['Value1','Value2','Value3']")

# copy() method : creates a copy of pandas objects
vColumn = dataframe_obj["Column"].copy() 
# this will create a seprate copy of teh series,
# any operation on this object , will not impact teh original object

####################### Text Data ##################################################

# String methods .lower() , .upper(), .title(), .len()
"Hello World".lower()  # "hello world"
"hello world".title # Hello World  : It capitalizes first letter of each world
len("1 am") # 4 :it counts spaces as well 

# note: to apply string method in pandas on series, it must use .str before applying any series methods
 
dataframe_obj["Column"].str.upper()  # otherwise if str is not used, it will throw error

# .str().replace(first_Parm, second_Param) # (replace,with) in the string
"hello world".replace('l','a') # heaao worad

dataframe_obj["Column"].str.replace("value_replace","with_value")

# filter with string methods, by creating boolean series using 

# .str.contains("Some_value"), .str.startswith("Some_value"), .str.endswith("Some_value")

mask = dataframe_obj["Column"].str.lower().str.contains("some_value") # it will generat ea boolean series
dataframe_obj[mask] # will return the rows only that returns True

# strip(), lstrip(), rstrip() # removes spaces from the string
"     hello world".lstrip() # "hello world"
"hello world   ".rstrip() # "hello world"
"     hello world   ".strip() # "hello world"

dataframe_obj["Column"].str.strip()

# strip() onn index
dataframe_obj.index = dataframe_obj.index.str.strip().str.title()

# strip() on columns
dataframe_obj.columns = dataframe_obj.columns.str.strip().str.title()

# split("delimiter") : it splits teh string in multple items in a list

"Hello new world".split(" ")  # retuns = ["Hello", "new", "World"]

dataframe_obj[column].str.split(",")

# get() on string : it puuls a component from list based on index
dataframe_obj[column].str.split(",").get(0).str.title().value_counts()

# split() with parameters, expand and n
# resut of split() with expand=True is a data frame if items than list of items
# it can be asigned to columns in a data frame

dataframe_obj[["Column_1","Column_2"]] = dataframe_obj["Column"].str.split(",", expand=True)

# parameter n limits the split() to n splits, than unlimited in Data frame

dataframe_obj[["Column_1","Column_2"]] = dataframe_obj["Column"].str.split(",", expand=True, n=1)  # it means only 1 split (first delimiter only)


######################## MultiIndex ##################################################
# multiindex : multi layered index. allows effective categorization of data

# set index with set_index() method on dataframe
dataframe_obj.set_index(keys = "Column_name")  # its sets the Column_name as index for teh data frame

# multiindex use list of column to set Keys
dataframe_obj.set_index(keys = ["Column_1","Column_2"])  # its sets the Column_1 and Column_2 as indexes for the data frame in teh same order column are mentioned in teh set_index method.

# sort_index()   : sorts the data frame records based on all the indexes of teh data frame.
dataframe_obj.sort_index()
dataframe_obj.sort_index(inplace=True)


# can be set multi index this way as well while importing file
dataframe_obj = pd.read_csv(file.csv, parse_dates["Date column Name"], index_col= ["Column_1", "Column_2"])
dataframe_obj.sort_index(inplace=True)

dataframe_obj.sort_index(ascendeing=True)  # sorts all indexes in ascending order

# custom sort, different sort in multiindex
dataframe_obj.sort_index(ascendeing = [True, False]) # sorts first index in ascending and second index in descending order

#------------------------------------------------------------------------------------
dataframe_obj.index  # will retunr multiindex object, list of index values in seperate lists within a list

dataframe_obj.index.names # will return the column names used in the indexes.

# .get_level_values()  : used to get vaues based on multiindex object in data frame
dataframe_obj.index.get_level_values(0)  # returns list of all index from first index column
dataframe_obj.index.get_level_values("Column1")  # returns list of all index from  index column column1

dataframe_obj.index.get_level_values(1)  # returns list of all index from second index column

# set_names() ; it is called on index object of data frame
# its changes the name of the index columns
dataframe_obj.index.set_names(["Column_1_NewName","Column_2_NewName"], inplace=True)
dataframe_obj.index.set_names(["Column_1_NewName","Column_2_SameName"], inplace=True)


# extract rows: in multiindex object using .loc[] and .ix[] methods
dataframe_obj.loc[tuple]  # need to pass tuple of index labels

dataframe_obj.loc[("label_index1")] # extract all values based on index 1 label
dataframe_obj.loc[("label_index1","label_index2")] # extract all values based on index 1 label and index 2 label

dataframe_obj.loc[("label_index1","label_index2"), "Column"]  # it will fetch teh value from teh particular column based for the index label 1 and index labe 2

dataframe_obj.ix[("label_index1","label_index2"), "Column"]
or 
dataframe_obj.loc[("label_index1","label_index2"), index_of_the_colum]

# transpose() on multiindex : it swaps the axis
dataframe_obj.transpose()  # it does not the impact the original data frame, but can be assigned to new data frame

dataframe_obj_new/same = dataframe_obj.transpose()

# ix() on transposed data frame
# ix["row_labels", ("Column Labels")]
dataframe_obj_new.ix["Column{row_now}", ("label_index1","label_index2")]
dataframe_obj_new.ix[("Column1{row_now}","Column2{row_now}"), ("label_index1","label_index2")]

# swaplevel() on multiindex : it swaps the positions of the indexes
dataframe_obj = dataframe_obj.swaplevel() # swaps the index col1 position with index col2

# stack() # takes teh column indexes and moves to left indexes. it stacks the coulmns in horizontal based index.
ex: dataframe_obj.stack()  # multi-index series type

# to_frame() converts series to data frame

series_obj.to_frame()
dataframe_obj.stack().to_frame()


# unstack() : it takes rows index to column index

dataframe_obj.unstack()
dataframe_obj.unstack().unstack()  # multiindex on columns

dataframe_obj.unstack(0) # moves first row index to column index
dataframe_obj.unstack(1) # moves second row index to column index
dataframe_obj.unstack(2) # moves third row index to column index

dataframe_obj.unstack(-1) # moves last row index to column index

dataframe_obj.unstack("Row_Index_Name") # moves this row ndex to column index 

# multi unstack index
dataframe_obj.unstack([1,0]) # moves these row ndex to column index 
dataframe_obj.unstack([rowindex2,rowindex1]) # moves these row ndex to column index


dataframe_obj.unstack("RowIndex1", fill_value = 0) # fills with 0 if no value found

# pivot() ; long to wide
dataframe_obj.pivot(index = "Column1", columns = "Column2", values = Column3)

# pivot() : as excel pivot
dataframe_obj.pivot_table(values = Column3, index = ["Column1", "Column2"], aggfunc="sum")

dataframe_obj.pivot_table(values = Column3, index = ["Column1", "Column2"], columns="column4"  aggfunc="sum")

dataframe_obj.pivot_table(values = Column3, index = ["Column1", "Column2"], columns=["column4","column5"]  aggfunc="sum")

or
# direct on the pandas object
pd.pivot_table(data = dataframe_obj, values = Column3, index = ["Column1", "Column2"], columns=["column4","column5"]  aggfunc="sum")


# melt() : reverse to pivot_table() : wide to long
# id_vars : are the columns to keep in the final table
# value_vars : columns that need to transposed into rows, if not mentioned then allremaining column other than id_vars  will be transposed in rows.
# var_name : Desired name to the column that has transposed columns as rows
# value_namme : Desired name to the value column

pd.melt(dataframe_obj, id_vars="Column_to_keep")
pd.melt(dataframe_obj, id_vars="Column_to_keep", var_name = "Some_Name", value_name = "Some name to Value column")
pd.melt(dataframe_obj, id_vars="Column_to_keep", value_vars = ["clo1_name","clo2_name"], var_name = "Some_Name", value_name = "Some name to Value column")



######################## Group By ##################################################
# groupby()
dataframe_obj  # data frame object
dataframe_obj.groupby("Column_Name") # it will give DataFrameGroupBy object

dataframe_obj["Column_Name"].nunique()
#will return same as 
len(dataframe_obj.groupby("Column_Name"))

# size() on group by object
var = dataframe_obj.groupby("Column_Name")

var.size()  # sorts bydefault on grouping name
# its same as
dataframe_obj["Column_Name"].value_counts()  # it willsorted based on counts.

# first() and last() on groupby object to get soem sample rows
var.first()
var.last()

# groups attribute : it gives python dictionary where keys aer group name an dvalue is the row index that are included in te group

var.groups

# get_group() on group by objects
var,get_group("Group_name")  # it return a data frame for teh selected group

# max(), min() Methods in groupby object
var.max()   # it will return the last row (alphabticaly) from the most left column from all the group
var.min()   # it will return the first row (alphabticaly) from the most left column from all the group
var.sum()  # it applies to numbers, floats. it applies only to numeric columns by groups
var.mean() # it gives mean/ average on numeric columns by groups

var.agg() # different action on different columns

# different action on different columns, by passing python dictionary
var.agg({"Column1": "sum",
         "Column2": "sum",
         "Column3" : "mean"})

# multiple actions on all columns by passing python list
var.agg(["size","sum", "mean"])

# different action on different columns, by passing python dictionary
var.agg({"Column1": ["size", "sum"]
         "Column2": "sum",
         "Column3" : "mean"})

# groupby multiple columns
dataframe_obj.groupby(["Column_Name","Column_Name2"])  # it will give multi index series


# iterate through groups

for loop and append()


######################## Merge, join, concatenate ######################################

# pd.concat() method. it is called on pandas library
 
pd.concat([Dataframe1, Dataframe2]) # it will just concatenate both dataframes keeping their index intact
pd.concat([Dataframe1, Dataframe2], ignore_index=True) # it will concatenate both dataframes and create a new index starting from 0
pd.concat([Dataframe1, Dataframe2], keys = [DF1, DF2])  # it will create a multiindex dataframe to identify data from different dataframes

var = pd.concat([Dataframe1, Dataframe2], keys = [DF1, DF2])
var.ix[DF1]   # it will give dataframe from dataframe 1 data
var.ix[("DF1","some_index")]
var.ix[("DF1","some_index"), "Column"]

# append()
Dataframe1.append(Dataframe2)  # it appends dataframe 2 to dataframe 1 as concatenate method keeping index frm both
Dataframe1.append(Dataframe2, ignore_index=True) # it will concatenate both dataframes and create a new index starting from 0


# merge() method is ussed to join data frames

# inner joins on single column
dataframe1.merge(dataframe2, on = "CommonColom_name")  # column name must be same in both the data frames on which join is applied 
# it will have columns(suffixed with X and Y) froom both data frames by default and wont remove any duplicate values

dataframe1.merge(dataframe2, on = "CommonColom_name", suffixes = ["-DF1", "-DF2"]) # this will have the columns suffed as -DF1 and -DF2 than X and Y

# inner joins on multiple columns
dataframe1.merge(dataframe2, on = ["CommonColom_name1", "CommonColom_name2"])

# outer join
dataframe1.merge(dataframe2, how="outer", on = "CommonColom_name", suffixes = ["-DF1", "-DF2"]) # column name must be same in both the data frames on which join is applied
dataframe1.merge(dataframe2, how="outer", on = "CommonColom_name", suffixes = ["-DF1", "-DF2"], indicator = True)

# left or right join
dataframe1.merge(dataframe2, how="left", on = "CommonColom_name")
dataframe1.merge(dataframe2, how="left", on = "CommonColom_name", sort=True)  # sorts by the common column join apied on

# if want to modify teh data frame, need to assign back to teh dataframe
dataframe1 = dataframe1.merge(dataframe2, how="left", on = "CommonColom_name", sort=True) 



# join when the column names vary using left_on, right_on
dataframe1 = dataframe1.merge(dataframe2, how="left", left_on = "DF1_Column", right_on = "DF2_Column")  # it will return ne both columns in teh return data farame

# to drop additional join column using drop()
dataframe1 = dataframe1.merge(dataframe2, how="left", left_on = "DF1_Column", right_on = "DF2_Column").drop("DF2_Column")

# Join Datframe1 and Datframe 2 based on Column from DF1, Index frm DF2
dataframe1 = dataframe1.merge(dataframe2, how="left", left_on = "DF1_Column", right_index = True)

# join using indexes only from both data frames
dataframe1.merge(dataframe2, how="left", left_index=True, right_index=True)

# .join : when 2 dataframes share same indexes, it can be used to join vertically
dataframe1.join(dataframe2)

# merge() on pandas library
pd.merge(dataframe1, dataframe2, how="left", left_on = "DF1_Column", right_on = "DF2_Column")



######################## Dates and Times ##################################################

#--------------------------- Python Date and Time functions-----------------#
import datetime as dt

someday = dt.date(209,4,12)  # year, month, day

str(someday)  # human readable format

someday.year  # return the year from teh date
someday.month  # return the month from teh date
someday.day  # return the day from teh date


dt.datetime(209,4,12, 8, 13, 57) # year, month, day , hour. minute, seconds
sometime = str(dt.datetime(209,4,12, 8, 13, 57))

sometime.hour
sometime.minute
sometime.second
# ------------------------------------------------------------------------------------

# pandas Timestamp objects
# no need to provide str() to pandas Timestamp object

pd.Timestamp("2015-03-31")  # it will be defaulted to mid night 00:00:00
pd.Timestamp("2015/03/31")
pd.Timestamp("2015,03,31")
pd.Timestamp("03/31/2019")
pd.Timestamp("31/03/2019")

pd.Timestamp("4/3/2019")   # m/d/y

pd.Timestamp("31/03/2019 08:35:15")
pd.Timestamp("31/03/2019 08:35:15 PM")
pd.Timestamp("31/03/2019 20:35:15")

# passing python datetime to pandas timestamp
pd.Timestamp(dt.datetime(2019,4,12, 8, 13, 57))

# pandas DateTimeIndex object, main use of this is to create index for some series or data frame
dates = ["2016-01-02", "2016-04-12", "2009-09-07"]

pd.DateTimeIndex(dates)   # it will convert teh strings into pandas Timestamp objects and store as Datetimeindex objects

dates = [dt.datetime(2016-01-02), dt.datetime(2016-04-03), dt.datetime(2015-05-02)]
dtIndex = pd.DateTimeIndex(dates)

# create series using datetime index for index
values = [10,20,30]
pd.Series(data=values, index=dtIndex)


# pd.to_datetime() : converts to pandas timestamp or datetimeindex objects
# it is mostly used to convert pandas series into timestamp objects
pd.to_datetime("2001-01-20")
pd.to_datetime(dt.date(2015,01,12))
pd.to_datetime(dt,datetime(2019,4,12, 8, 13, 57))
pd.to_datetime(["2001-01-20", "31/03/2019 ", 2016", "July 4th 1996"])

times = pd.Series(["2001-01-20", "31/03/2019 ", 2016", "July 4th 1996"])
pd.to_datetime(times)


# when teh series has bad data
times = pd.Series(["2001-01-20", "31/03/2019 ", 2016", "July 4th 1996", "Hello"])
pd.to_datetime(times) # will throw error, cause Hello cannot be converted into timestamp

pd.to_datetime(times, errors = "coerce") # will replace the items with Nat (Not a timestamp) when its not convertable to timestamp

# deal with unix time # it stores time in seconds since jan 1917
pd.to_datetime([1349720105,1349806505], unit = "s")  # its convert into the pandas timestamp


# pd.date_range()  # allows to generate date reanges
pd.date_range()  # atleast 2 parametrs are required

pd.date_range(start = "2016-01-01", end = "2016-01-10", freq="D")  # generates a datetimeindex of TimeStamp objects, from start to end with interval of 1 day
pd.date_range(start = "2016-01-01", end = "2016-01-10", freq="1D") # its same

pd.date_range(start = "2016-01-01", end = "2016-01-10", freq="2D")  # now te interval is 2 days

# frequencies
D : regular calendar days
B : business days / working days
W : week, starts from Sunday as default
W-FRI : to start from a particular weekday
H : hour
M : month-ends-> start of each month
MS : month-start -> first day of each month
A : year-end -  last date of the years
A-MAY : last day of may as last day of Year

# number of periods to generate, using periods parameter
pd.date_range(start = "201-01-21", periods=25, freq="D")  # it will gerate 25 items with interval of 1 day

pd.date_range(start = "201-01-21", periods=25, freq="B")  # it will gerate 25 business days starting from startdate

pd.date_range(end = "201-01-21", periods=25, freq="B")  # it will gerate 25 business days ending on end date

# .dt accessor : its similar to .str  with string methods
bunch_of_dataes = pd.date_range(start = "2000-01-01", end="2010-12-31", freq="24D")

s = pd.Series(bunch_of_dataes)
# extract day
s.day # will throw error, need to use .dt accessor

s.dt.day  # it will generate a series of days from each element
s.dt.month  # it will generate a series of month from each element
s.dt.weekday_name  # it will generate a series of weekdays name from each element
s.dt.is_quarter_start  # it will generate a series of boolean from each element

# pandas-datareader  : it is used to fetch dat afrom online source
conda install pandas-datareader

# import  financial dataset from online
import pandas as pd
import datetime as dt
from pandas_datareader import data

data.DataReader()   # it queries the online information from internet

company = "MSFT"
start = "2019-01-01"
end = "2020-03-31"
stocks = data.DataReader(name=company, data_source="yahoo", start=start, end=end)

# attibutes
stocks.values
stocks.columns
stocks.index[0]
stocks.axes  # combines Row index labels and Column labels

# extract rows from dta frame
stocks.loc["index-label"]
stocks.iloc[index-number]
stocks.ix["index-label"]
stocks.ix[index-number]

stocks.loc["index-label" : "index-labe2"]
stocks.ix["index-label" : "index-labe2"]

# date_range() : use it to create values at yearly interval
myDates = pd.date_range(start = "somedate", end="somedate", freq = pd.DateOffset(years = 1)) # it will generate a DateTimeIndex

mask = stocks.index.isin(myDates)  # willgenerate boolean series
stocks.loc[mask] 
or
stack[mask]]

# Timestamp object attributes
someday = stack.index[500]  # will return some timestamp obkject, since the index labels are dates

someday.day
someday.month
someday.year
someday.weekday_name
someday.is_month_end
someday.is_month_start


# insert() : to insert a column in teh dataframe
stocks.insert(ColumnIndex, Column_Name, stocks.index.weekday_name)

# truncate() on dataframe, to extract values from a date to soem date
stocks.truncate(before= "index_Label", after = "Index_label2")  # its to bring the set of values, slicing

# DateOffset() methods on pandas
stocks = data.DataReader(name="GOOG", data_source="google", start=dt.date(2000,1,1), end=dt.datetime.now())

stocks.index + 5 # will throw error, number cannot be simpply added to Timestamp

stocks.index + pd.DateOffset(days = 5)
stocks.index + pd.DateOffset(weeks = 2)
stocks.index - pd.DateOffset(months = 2)
stocks.index + pd.DateOffset(years = 1)
stocks.index + pd.DateOffset(hours = 2)

stocks.index - pd.DateOffset(years=1, months = 2, dayss = 3)
or 
stocks.index - pd.DateOffset(months = 2, years=1, dayss = 3)

# tseries attributes
pd.tseries.offsets.MothEnd()

stocks.index + pd.tseries.offsets.MotnhEnd()  # next month end
stocks.index - pd.tseries.offsets.MotnhEnd()  # previous month end

stocks.index + pd.tseries.offsets.MotnhBegin()  # next month end
stocks.index - pd.tseries.offsets.MonthBegin()  # previous month en

# pd.tseries.offsets.  can be imported directly as library from pabdas
from pandas.tseries.offsets import *

stocks.index - MonthBegin()  # can be now written in this way
stocks.index + BMonthEnd()  # next business days
stocks.index - BMonthEnd() # prior business day
stocks.index + QuarterEnd() # next available Quarter end
stocks.index udem+ BQuarterStart() # next available Quarter business start
stocks.index + YearEnd() # next year end


# timestamp  : represents a particular moment

# Timedelta object: it represents a duration
timeA = pd.TimeStamp("2016-03-31")
timeB = pd.TimeStamp("2016-03-20")

timeA - timeB  # gives TimeDelta object  with differencce  11 days 0:000:00

timeB - timeA  # it will give negative duration Timedata object

# create Timedelta object using Timedelta(). Parameters same as DateOffset()
pd.Timedelta(days=3)  # creates a duration of 3 days
pd.Timedelta(days=3, minutes=45, hours = 2, weeks = 8)

# years parameter doesnot work with Timedelta()

# also it works this way
pd.Timedelta("3 days")
pd.Timedelta("14 days 6 hours 12 minutes 49 seconds")  # weeks does not work this way


######################## Input output ##################################################
# read_csv() url argument : to load data directly from internet
url = "http...../file.csv"

dataframeobj = pd.read_csv(url)

# convert Series to List
dataframe[Column].tolist()   # it wil generate python list from teh series
dataframe[Column].to_frame()   # it wil convert series into dataframe

",".join((name) for name in dataframe[Column])   # lambda style writing join method
# it will generate python string

# export data frame into CSV using to_csv()
dataframe.to_csv("filename.csv")
# by default it will include teh index in export, use index=Flase to exclude the index column from the extract
dataframe.to_csv("filename.csv", index=False)

# export selective column from teh dataframe into csv file
dataframe.to_csv("filename.csv", index=False, columns=["column_1","column_2"])

# export encoded characters from te data frame
dataframe.to_csv("filename.csv", index=False, columns=["column_1","column_2"], encoding ="utf-8")


# import / export excel file
conda install xlrd
conda install openpyxl

# import excel in dataframe, by default pandas import first sheet from teh excel
dataframeObj = pd.read_excel("file.xlsx")

# multiple sheets in the excel. can be provided sheet name or nummeric position starting from 0 of teh sheet
dataframeObj = pd.read_excel("file.xlsx", sheetname = "Shetname")
dataframeObj = pd.read_excel("file.xlsx", sheetname = 1)

# or can be imported multiple sheets, give list of sheet names or sheet indexes
dataframeObj = pd.read_excel("file.xlsx", sheetname = [0, 1])  # it will be now dictionary of data frames
dataframeObj = pd.read_excel("file.xlsx", sheetname = ["Shetname1", "Shetname2"])  # it will be now dictionary of data frames
# 1 data frame for each sheet
# keys of teh dictionary will be the Sheet index or Sheet name , based on the sheetname parametere in the read_excel() method


# automate the importing of all work sheets withot providing teh name of each sheet in teh list
# it will create teh dict keys based on teh Sheet names in teh excel file
dataframeObj = pd.read_excel("file.xlsx", sheetname = None)

# export data into excel ExcelWriter()
# Step 1: create the instance
excel_fileObj = pd.ExcelWriter("FileName.xlsx")

# Step 2: pass teh ExcelWriter object to the to._excel()
dataframeObj_1.to_excel(excel_fileObj, sheet_name = "Girls", index = False)
dataframeObj_2.to_excel(excel_fileObj, sheet_name = "Girls", index = False, columns = ["Column1", "COlumn2"])

# step 3: save() the excel file, that will create the excl file with the sheets
excel_fileObj.save()



######################## Visualization ##################################################

# matplotlib
import matplotlib.pyplot as plt

# with jupyter notebook
%matplotlib inline

# plot() : it is used to plot charts from data frame
dataframeObj.plot()  
# by default plots all Coumns on Y-Axis, creating a series for each column
# by default plots Index as X-axis albels

# plot() parameters
dataframeObj.plot(y = "Column Name") # plots the column against Y axis

or

dataframeObj["Column"].plot()  # gives te same result as plotting one column from teh data frame

dataframeObj[["Column","Column2"]].plot()  # Plots  both the columns on te chart against Y-Axis, using index as X-axis


# asthetic ; beautification of chart
plt.style.available # provides teh available styles

plt.style.use("fivethirtyeight") # it will apply the style to the notebook, so need to change on te individual chart if want to change
dataframeObj.plot(y= "Column")

plt.style.use("dark_background") # it will apply the style to the notebook, so need to change on te individual chart if want to change
dataframeObj.plot(y= "Column")


plt.style.use("ggplot") # it will apply the style to the notebook, so need to change on te individual chart if want to change
dataframeObj.plot(y= "Column")

# Bar chart
plt.style.use("ggplot")
dataframeObj["Column"].apply(Custom_Dunction).value_counts().plot(kind = "bar")  # it plots teh vertical Bar Graph. use "barh" to plot horizontal bar graph


# pie chart
plt.style.use("ggplot")
dataframeObj["Column"].apply(Custom_Dunction).value_counts().plot(kind = "pie")  # it plots teh pie chart. 
dataframeObj["Column"].apply(Custom_Dunction).value_counts().plot(kind = "pie", legend = True)  # it enables te legends on teh chart

# histograms
plt.style.use("ggplot")
unique_Count = dataframeObj["Column"].apply(Custom_Dunction).nunique() # it will give the counts of unique bucket
dataframeObj["Column"].apply(Custom_Dunction).plot(kind = "hist", bins = unique_Count)  # it plots teh histogram chart. 


######################## options and settings ###########################################
import pandas as pd
import numpy as np

# numpy can be used to crate random array
np.random.randint(0, 100, [1000, 50])  # generates random integer from 0 to 100, 1000 rows and 50 columns

data = np.random.randint(0, 100, [1000, 50]) 
# np.random.randn() # it generates random number including floats

# DataFrame() # converts data into dataframe object
pd.DataFrame(data)
df = pd.DataFrame()   # create empty dataframe object

# multiple ddataframes can be combined into 1
df = df.append(df2, ignore_index='True')

dfnew = pd.DataFrame(pd.np.empty((0,4)))
# This creates an empty pandas DataFrame with 0 rows and 4 columns.

new_column = pd.Series([ticker, company_name, stock_price, dividend_yield])
output_data = output_data.append(new_column, ignore_index = True)
# adds data into columns into teh dataframe
output_data.columns = ['Ticker', 'Company Name', 'Stock Price', 'Dividend Yield']
# asasign column names

# pandas options
# ddefault number of rows
pd.options.display.max_rows  # default is 60
pd.options.display.max_rows=18  # dchanges teh difault number of displayed rows to 18

# default number of columns
pd.options.display.max_columns  # default is 20
pd.options.display.max_columns = 4 # updates teh default to 4 columns

# change options with Padans official methods
# get_option(), set_option(), reset_option(), describe

# rows
pd.get_option("max_rows")  # gives teh defult set number of max rows
pd.set_option("max_rows", 20)  # sets teh defult set number of max rows to 20

# columns
pd.get_option("max_columns")  # gives teh defult set number of max columns
pd.set_option("max_columns", 20)  # sets teh defult set number of max columns to 20

# reset_options()  # it takes the option and reste it to default
pd.reset_option("max_rows")
pd.reset_option("max_columns")


# describe option: it describes the option in pandas
pd.describe_option("max_rows")  # it describes teh option

# precision option : it is used with decimal
pd.get_option("precision")  # 6 is teh default positions after decimal



######################## Environments ##################################################

######################## Environments ##################################################

######################## Environments ##################################################

######################## Environments ##################################################

######################## Environments ##################################################

######################## Environments ##################################################

######################## Environments ##################################################

######################## Environments ##################################################
